{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21dcae54-a1a7-45b3-847c-93630caa0270",
   "metadata": {},
   "source": [
    "# Reconstruction Storage (HDF5 File Making) \n",
    "\n",
    "In this notebook, we will:\n",
    " * Describe how to generate **`.h5`** format files to store reconstruction outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4c6a3ec-6b7d-4693-a29c-9649fd29d571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/08\n"
     ]
    }
   ],
   "source": [
    "# Basic boilerplate imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml, os, sys, re\n",
    "\n",
    "# Visualization Tools\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set lartpc_mlreco3d path\n",
    "LARTPC_MLRECO_PATH = \"/sdf/group/neutrino/koh0207/lartpc_mlreco3d/\"\n",
    "sys.path.append(LARTPC_MLRECO_PATH)\n",
    "from mlreco.main_funcs import process_config, cycle\n",
    "from mlreco.iotools.factories import loader_factory\n",
    "from mlreco.trainval import trainval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac12aade-23ca-43d0-9b41-9a1869bd9ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Config processed at: Linux sdfampere001 4.18.0-372.32.1.el8_6.x86_64 #1 SMP Fri Oct 7 12:35:10 EDT 2022 x86_64 x86_64 x86_64 GNU/Linux\n",
      "\n",
      "$CUDA_VISIBLE_DEVICES=\"0\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load configuration file of the ML chain\n",
    "config_path = './hdf5_example.cfg'\n",
    "config = yaml.load(open(config_path, 'r'), Loader=yaml.Loader)\n",
    "process_config(config, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59b60cff-ea59-44f6-a929-c20169780ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sdf/home/k/koh0207/notebooks/workspace/reco_and_visualization'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1b59294-e89f-4bab-9f23-0b1cf9e9a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set event_list if necessary\n",
    "event_list = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ab73d-563c-44ba-a67c-e67af3f8940d",
   "metadata": {},
   "source": [
    "## 1. Storing reconstruction outputs to HDF5.\n",
    "\n",
    "Once our ML model is fully trained and deployed, we may set our model to test mode and make use of its predictions: track vs shower separation, particle clustering, and PID to name a few. However, once we start using the same saved model  (parameters), there is no need for repeatedly running forward operations of the network to obtain the same predicted labels. This is especially true as most analysis level tasks involve using the same predictions from the ML model but with different post-processing algorithms for calorimetry, selection, and tagging. As a solution, we save all reconstruction related ML model outputs to [Hierarchical Data Format (HDF)](https://www.hdfgroup.org/solutions/hdf5/). This is to:\n",
    " * Avoid unnecessary usage of GPU resources\n",
    " * Shorten the test-debug-rerun process of doing physics analysis with `lartpc_mlreco3d`\n",
    " \n",
    "We first illustrate how to run the ML chain and save its outputs to HDF5 (`.h5`) file format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92fa7c26-0e1c-46a0-b945-6cab4d28e0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: /sdf/data/neutrino/icarus/mpvmpr/test/all_0.root\n",
      "Loading tree sparse3d_reco_cryoE\n",
      "Loading tree sparse3d_reco_cryoE_chi2\n",
      "Loading tree sparse3d_reco_cryoE_hit_charge0\n",
      "Loading tree sparse3d_reco_cryoE_hit_charge1\n",
      "Loading tree sparse3d_reco_cryoE_hit_charge2\n",
      "Loading tree sparse3d_reco_cryoE_hit_key0\n",
      "Loading tree sparse3d_reco_cryoE_hit_key1\n",
      "Loading tree sparse3d_reco_cryoE_hit_key2\n",
      "Loading tree sparse3d_pcluster_semantics_ghost\n",
      "Loading tree sparse3d_pcluster\n",
      "Loading tree particle_corrected\n",
      "Loading tree cluster3d_pcluster\n",
      "Loading tree particle_pcluster\n",
      "Loading tree particle_mpv\n",
      "Loading tree sparse3d_pcluster_semantics\n",
      "Loading tree cluster3d_sed\n",
      "Loading tree neutrino_mpv\n",
      "Found 10475 events in file(s)\n",
      "Shower GNN: True\n",
      "Track GNN: True\n",
      "Particle GNN: False\n",
      "Interaction GNN: True\n",
      "Kinematics GNN: False\n",
      "Cosmic GNN: False\n",
      "\n",
      "            Since one of the GNNs are turned on, process_fragments is turned ON.\n",
      "            \n",
      "\n",
      "        Fragment processing is turned ON. When training CNN models from\n",
      "         scratch, we recommend turning fragment processing OFF as without\n",
      "         reliable segmentation and/or cnn clustering outputs this could take\n",
      "         prohibitively large training iterations.\n",
      "        \n",
      "Restoring weights for  from /sdf/group/neutrino/drielsma/train/icarus/localized/full_chain/weights/full_chain/grappa_inter_nomlp/snapshot-2999.ckpt...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "loader = loader_factory(config, event_list=event_list)\n",
    "dataset = iter(cycle(loader))\n",
    "Trainer = trainval(config)\n",
    "\n",
    "# Please check the \"Done.\" message that ensures the weights are loaded properly!\n",
    "loaded_iteration = Trainer.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200d8b34-49cd-42fa-b4d1-7b51a4560c6a",
   "metadata": {},
   "source": [
    "### 1-1. Data and Result dictionary format\n",
    "\n",
    "The following line runs one forward operation of the ML chain. It has two outputs:\n",
    " * `data`: Python dictionary containing inputs to the network (3d spacepoints, and deposition values) and any truth information used for labels. It also includes meta data information such as the image index number and px to cm conversion factor. \n",
    " * `result`: Python dictionary storing outputs of the ML chain. Any quantity that involves some amount of reconstruction will be stored in the `result` dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "254cd12f-e040-4791-a1f5-b7c4e0bf29d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deghosting Accuracy: 0.9879\n",
      "Segmentation Accuracy: 0.9949\n",
      "PPN Accuracy: 0.9759\n",
      "Clustering Accuracy: 0.9761\n",
      "Clustering Edge Accuracy: 0.6996\n",
      "Shower fragment clustering accuracy: 0.9696\n",
      "Shower primary prediction accuracy: 1.0000\n",
      "Track fragment clustering accuracy: 1.0000\n",
      "Interaction grouping accuracy: 0.8547\n",
      "Particle ID accuracy: 0.9706\n",
      "Primary particle score accuracy: 0.9706\n"
     ]
    }
   ],
   "source": [
    "data, result = Trainer.forward(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4b4636f-3f32-4fed-9e0a-0bc19d81262c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Data Dictionary = 11\n",
      "Length of Result Dictionary = 135\n"
     ]
    }
   ],
   "source": [
    "print('Length of Data Dictionary =', len(data))\n",
    "print('Length of Result Dictionary =', len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24418f00-f11c-441a-8d27-50ddab699043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sdf/home/k/koh0207/notebooks/workspace/reco_and_visualization'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186024f4-810c-4c14-8307-9b350d0f5c24",
   "metadata": {},
   "source": [
    "### 1-2. Saving reconstruction outputs to HDF5\n",
    "\n",
    "As such, the result dictionary contains a lot of information, for which only a subset of them are necessary for analysis. Most key, value pairs of the `result` dictionary are intermediate outputs from each stage of the reco chain. These are neceesary for the chain to operate and for debugging intermediate stages separately. \n",
    "\n",
    "Hence, we only save a subset of the `data/result` dictionary to HDF5 by selecting out the key names. We can do this under the `iotools.writer` section of the chain configuration file:\n",
    "```yaml\n",
    "# IO configuration\n",
    "iotool:\n",
    "  batch_size: 1\n",
    "  shuffle: False\n",
    "  num_workers: 4\n",
    "  #collate_fn: CollateSparse\n",
    "  collate:\n",
    "    collate_fn: CollateSparse\n",
    "  ...\n",
    "  ...\n",
    "  ...\n",
    "  writer:\n",
    "    name: HDF5Writer\n",
    "    file_name: $PATH/output.h5\n",
    "    input_keys:\n",
    "      - index\n",
    "      - meta\n",
    "      - cluster_label\n",
    "      - particles_asis\n",
    "      - sed\n",
    "    result_keys:\n",
    "      - input_rescaled\n",
    "      - cluster_label_adapted\n",
    "      - segmentation\n",
    "      - ppn_points\n",
    "      - ppn_coords\n",
    "      - ppn_masks\n",
    "      - ppn_classify_endpoints\n",
    "      - fragment_clusts\n",
    "      - fragment_seg\n",
    "      - shower_fragment_node_pred\n",
    "      - shower_fragment_start_points\n",
    "      - track_fragment_start_points\n",
    "      - track_fragment_end_points\n",
    "      - particle_clusts\n",
    "      - particle_seg\n",
    "      - particle_start_points\n",
    "      - particle_end_points\n",
    "      - particle_group_pred\n",
    "      - particle_node_pred_type\n",
    "      - particle_node_pred_vtx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6047d24a-7a9c-48ed-953f-e332bfdc26f7",
   "metadata": {},
   "source": [
    " * `name`: type of `writer` to be used to store model outputs, for which there is only one option (namely `HDF5Writer`). \n",
    " * `file_name`: full path to output `.h5` file.\n",
    " * `input_keys`: keys in the `data` dictionary that will be stored to `output.h5`.\n",
    " * `result_keys`: keys in the `result` dictionary that will be stored to `output.h5`. \n",
    " \n",
    "> **Warning**\n",
    "> : Note that depending on the data that you will be working on, the necessary keys needed for analysis will differ. The above example was taken from a MPVMPR validation configuration file. \n",
    "\n",
    "After adding the `writer` field to your chain config, you can launch a iteration loop to generate your HDF5 file:\n",
    "```bash\n",
    "> python3 $PATH_TO_LARTPC_MLRECO3D/bin/run.py $PATH_TO_CHAIN_CONFIG\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f47a552-5397-4671-8afc-fca502a60437",
   "metadata": {},
   "source": [
    "### 1-3. Reading HDF5 files from the notebook.\n",
    "\n",
    "To read HDF5 files directly from the notebook, we will use the `HDF5Reader` class, for example:\n",
    "```python\n",
    "from mlreco.iotools.readers import HDF5Reader\n",
    "reader = HDF5Reader('output.h5')\n",
    "```\n",
    "Although HDF5 file reading is handled separately under analysis tools (which we will explain shortly afterwards), it is still good practice to check if the appropriate quantities are being stored properly by opening the `.h5` files directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df01fdab-08cd-4dbb-8657-12c8ef4daafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlreco.iotools.readers import HDF5Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26fa16c3-1e5e-4288-826c-e10f72979df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered ./output.h5\n"
     ]
    }
   ],
   "source": [
    "reader = HDF5Reader('./output.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b12030e-053c-4188-9177-2f44705ce6df",
   "metadata": {},
   "source": [
    "The first index of the reader is the batch index. Since we generated 10 events, it should range from 0 to 9. The second index is a placeholder for the `data` and `result` dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "716038bc-fdd2-49b3-872a-ac3f33af989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, result = reader[0][0], reader[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc2c34-a1b6-4429-8814-82008fd31d7a",
   "metadata": {},
   "source": [
    "While most data structures are stored as is, some of them are saved as \"blueprints\" to reduce the overall size of the h5 file. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
